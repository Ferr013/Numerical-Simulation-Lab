{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Numerical Simulation Laboratory (NSL) </span>\n",
    "## <span style=\"color:blue\">  Numerical exercises 1</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01.1\n",
    "- Test the Pseudo-Random Number generator downloaded from the NSL Ariel web site by estimating:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\langle r \\rangle = \\int_0^1 r dr = 1/2$. <font color=\"red\">Make a picture of the estimation of $\\langle r \\rangle$ and its uncertainty (which corresponds to Standard Deviation of the mean for the estimation of $\\langle r \\rangle$) as a function of the number of *throws*</font> (see below: Computing statistical uncertainties).\n",
    "2. $\\sigma^2 = \\int_0^1 (r-1/2)^2 dr = 1/12$. <font color=\"red\">Make a picture of the estimation of $\\sigma^2$ and its uncertainty (which corresponds to Standard Deviation of the mean for the estimation of $\\langle (r-1/2)^2 \\rangle$) as a function of the number of *throws*</font> (see below: Computing statistical uncertainties).\n",
    "\n",
    "The hypothesis is that the numbers $r$ are drawn from a uniform distribution. In Statistics we cannot prove that some random events are drawn from a particular distribution (Note, in fact, that such hypothesis is false: pseudo-random numbers are drawn from a deterministic algorithm!); we can try to estimate the probability that $r$ **are not** drawn from a uniform distribution. If this probability is low, we can safely reject this last hypothesis.\n",
    "<p style=\"border:2px; border-style:solid; border-color:#F5F5F5; padding: 1em; background-color:#F5F5F5\">\n",
    "Pearson's cumulative test statistic $\\chi^2$ is a measure of the error between observations, $O_i$, and expected values, $E_i$:\n",
    "$$\\chi^2 = \\sum_{i} \\frac{\\left( O_i - E_i \\right)^2}{E_i}$$\n",
    "The numerator is a squared distance between observations, $O_i$, and expected values, $E_i$, and thus should be compared (at the denominator) with the expected squared fluctuations (variance) of the relative distribution. Why variance = $E_i$? \n",
    "The probability of getting exactly $k$ (independent) successes, each one with probability $p$, in $n$ trials is given by the Binomial distribution (see <a href=\"https://en.wikipedia.org/wiki/Binomial_distribution\">this Wikipedia link</a>):\n",
    "$$Pr(X=k) = {{n}\\choose{k}} p^k (1-p)^{n-k} = \\frac{n!}{k! (n-k)!} p^k (1-p)^{n-k}$$\n",
    "The average of the Binomial distribution is $\\langle X \\rangle = np$, the variance is: $\\sigma^2 = np (1-p)$. Thus if $p$ is small we have that $\\sigma^2 \\simeq np$, that is $\\sigma^2 \\simeq \\langle X \\rangle$ and this explains the denominator $E_i$ in $\\chi^2$\n",
    "</p>\n",
    "\n",
    "3. Divide $[0,1]$ into $M$ identical sub-intervals and implement the $\\chi^2$ test. Obviously, the number of expected events observed in each sub-interval after $n$ *throws*, according to a uniform distribution, is $np = n\\times 1/M= n/M$. Fix $M=10^2$ and use for $n$ the first $10^4$ pseudo-random numbers, then the successive $10^4$ pseudo-random numbers, and so on ... 100 times. <font color=\"red\">Plot $\\chi^2_j$ for $j=1, ..., 100$</font>. In this case the chi-square statistic is:\n",
    "$$\\chi^2 = \\sum_{i=1}^M \\frac{\\left( n_i - n/M \\right)^2}{n/M}$$\n",
    "We should expect on average that $(n_i - n/M)^2 \\simeq n/M$ and thus $\\chi^2 \\simeq 100$, i.e. the number of sub-intervals.\n",
    "A larger value of $\\chi^2$ indicates that the hypothesis ($n_i$ are drawn from a uniform distribution) is rather unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01.2\n",
    "- Extend Pseudo-Random Number generator downloaded from the NSL Ariel web site and check the Central Limit Theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <font color=\"red\">Add two probability distributions</font> by using the **method of the inversion of the cumulative distribution** to sample from a **generic** <font color=\"red\">exponential distribution</font>, $p(x) = \\lambda \\exp(-\\lambda x)$, $x\\in [0;+\\infty]$ (see <a href=\"https://en.wikipedia.org/wiki/Exponential_distribution\">this Wikipedia link</a>), and a **generic** <font color=\"red\">Cauchy-Lorentz distribution</font> $p(x)=\\frac{1}{\\pi}\\frac{\\Gamma}{(x-\\mu)^2+\\Gamma^2}$, $x\\in [-\\infty;+\\infty]$ (see <a href=\"https://en.wikipedia.org/wiki/Cauchy_distribution\">this Wikipedia link</a>).\n",
    "2. <font color=\"red\">Make 3 pictures</font> with the histograms obtained filling them with $10^4$ realizations of $S_N = \\frac{1}{N}\\sum_{i=1}^N x_i$ (for $N=1, 2, 10, 100$), being $x_i$ a random variable sampled throwing a *standard* dice (fig.1), an *exponential* dice (fig.2, use $\\lambda=1$) and a *Lorentzian* dice (fig.3, use $\\mu=0$ and $\\Gamma=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01.3\n",
    "- **Simulate** the Buffonâ€™s experiment (see LSN_Lecture_00, supplementary material):  A needle of length $L$ is thrown at random onto a horizontal plane ruled with straight lines a distance $d$ ( $d > L$) apart. The probability $P$ that the needle will intersect one of these lines is: $P = 2L/\\pi d$. This could be used to evaluate $\\pi$ from throws of the needle: if the needle is thrown down $N_{thr}$ times and is observed to land on a line $N_{hit}$ of those times, we can make an estimate of $\\pi$ from\n",
    "$$\\pi = \\frac{2L}{Pd} = \\lim_{N_{thr} \\to \\infty}\\frac{2LN_{thr}}{N_{hit}d}$$\n",
    "<font color=\"red\">Make a picture of the estimation of $\\pi$ and its uncertainty (Standard Deviation of the mean) as a function of $N_{thr}$</font> (see below: Computing statistical uncertainties). If possible, do not use $\\pi$ to evaluate $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing statistical uncertainties: the blocking method\n",
    "- A fundamental step in every Monte Carlo calculation is the estimation of its statistical uncertainty, **it's a must!** Your task would not be completed if you had not calculated it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:2px; border-style:solid; border-color:#F5F5F5; padding: 1em; background-color:#F5F5F5\">\n",
    "    In computing the statistical uncertainties, i.e. the standard deviation of the mean, you need an estimation of the variance $\\sigma^2$, the second central moment. First of all note that the variance can be computed from\n",
    "    $$\\sigma^2_A := \\langle (A-\\langle A\\rangle)^2\\rangle = \\langle A^2 \\rangle -\\langle A\\rangle^2 $$\n",
    "    What is the best way to estimate $\\sigma^2_A$? Imagine that your Monte Carlo estimation of $A$ is obtained from a calculation which uses $M$ Monte Carlo \"steps\" (intentionally, here I am generic because what is a single \"step\" in a Monte Carlo calculation strictly depends on the specific calculation); you can always divide such $M$ Monte Carlo \"steps\" in $N$ blocks, with $N<M$. In each block, you can use your $M/N$ Monte Carlo \"steps\" to obtain an estimate of $A$, let me call it $A_i$ with $i=1,N$, and then you have also $A^2_i$ with $i=1,N$.\n",
    "    At this point everything becomes clear:\n",
    "    $$ \\langle A^2 \\rangle \\simeq \\frac{1}{N} \\sum_{i=1}^N A^2_i \\quad \\quad \\langle A\\rangle^2 \\simeq \\left( \\frac{1}{N} \\sum_{i=1}^N A_i \\right)^2 $$\n",
    "    and finally the statistical uncertainty with $N$ :\n",
    "    $$\\frac{\\sigma}{\\sqrt{N-1}} \\simeq \\sqrt{\\frac{1}{N-1} \\left[ \\frac{1}{N} \\sum_{i=1}^N A^2_i - \\left( \\frac{1}{N} \\sum_{i=1}^N A_i \\right)^2 \\right]} $$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a Monte Carlo calculation, you can use the arguments above to outputting an estimate of A and its statistical uncertainty on the fly, during your calculation. You will be able to do this every $M/N$ steps, and thus, in the end, $N$ times. Note that after the first $M/N$ Monte Carlo steps, at the end of the first block, your estimation of the uncertainty is not computable, so set it to zero and compute it only from the second block. *Question: How freedom you have for the choice of $N$?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
